{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"f_oFjJZ63g_o"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzWan00-3g_r"},"outputs":[],"source":["def RequestPage(Job,PageNumber):\n","    url=f\"https://wuzzuf.net/search/jobs/?a=hpb&q={Job}&start={PageNumber}\"    \n","    #print(url)\n","    response = requests.get(url)\n","    return response"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38nZkPT-3g_s","outputId":"8964ba9e-f49f-438e-c1c5-d875fc26fac3"},"outputs":[{"name":"stdout","output_type":"stream","text":["machine%20learning Started\n","1\n","2\n","3\n","machine%20learning Ended\n","data%20analysis Started\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","data%20analysis Ended\n","data%20science Started\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","data%20science Ended\n","business%20intelligence Started\n","1\n","2\n","3\n","4\n","5\n","business%20intelligence Ended\n"]}],"source":["Jobs =[\"machine%20learning\",\"data%20analysis\",\"data%20science\",\"business%20intelligence\"]\n","#\"machine%20learning\",\n","for job in Jobs:\n","    print(f\"{job} Started\")\n","    ParseJob(job)\n","    print(f\"{job} Ended\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0DggzAW3g_t"},"outputs":[],"source":["def ParseJob(JobName):\n","    #print(JobName)\n","    pageNumber=0;\n","    while True:\n","        response = RequestPage(JobName,pageNumber)        \n","        #response\n","        soup = BeautifulSoup(response.content, 'html')\n","        #soup                \n","        titles = soup.find_all(\"h2\", {'class': 'css-m604qf'})\n","        #print(titles)\n","        if(not titles):\n","            break\n","        pageNumber=pageNumber+1\n","        print(pageNumber)\n","        #print(titles)\n","        #for title in titles:\n","        #    print(title.text)\n","        titles_lst = [title.a.text for title in titles]\n","        #titles_lst\n","        links = [('https://wuzzuf.net' + title.a['href']).replace(' ', '%20') for title in titles]\n","        #links\n","        occupations = soup.find_all(\"div\", {'class': 'css-1lh32fc'})\n","        #occupations\n","        occupations_lst = [occupation.text for occupation in occupations]\n","        #occupations_lst\n","        locations = soup.find_all(\"span\", {'class': 'css-5wys0k'})\n","        #locations\n","        locations_lst = [location.text for location in locations]\n","        #locations_lst\n","        companies = soup.find_all(\"a\", {'class': 'css-17s97q8'})\n","        #companies\n","        companies_lst = [company.text for company in companies]\n","        companies_lst = [company.replace(' -', '') for company in companies_lst]\n","        #companies_lst\n","        specs = soup.find_all(\"div\", {'class': 'css-y4udm8'})\n","        specs_lst = [spec.text for spec in specs]\n","        jobsName_lst = [JobName.replace('%20',' ') for spec in specs]\n","        #print(jobsName_lst)\n","        #specs_lst\n","        #print(titles_lst)\n","        scraped_data = CreateTable(jobsName_lst,titles_lst,locations_lst,links,occupations_lst,companies_lst,specs_lst)\n","        scraped_data\n","        CSV = WriteToCSVFile(scraped_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6SshGvQ3g_u"},"outputs":[],"source":["def CreateTable(jobsName_lst,titles_lst,locations_lst,links,occupations_lst,companies_lst,specs_lst):\n","    scraped_data = {}\n","    scraped_data['Job'] = jobsName_lst    \n","    scraped_data['Title'] = titles_lst\n","    scraped_data['Location'] = locations_lst\n","    scraped_data['Link'] = links\n","    scraped_data['Occupation'] = occupations_lst\n","    scraped_data['Company'] = companies_lst\n","    scraped_data['Specs'] = specs_lst\n","    return scraped_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wg8huveZ3g_u"},"outputs":[],"source":["def WriteToCSVFile(scraped_data):\n","    import pandas as pd\n","    import os\n","    df = pd.DataFrame(scraped_data)\n","    # if file does not exist write header \n","    if not os.path.isfile('jobs.csv'):        \n","        df.to_csv('jobs.csv', index=False)\n","    else: # else it exists so append without writing the header        \n","        df.to_csv('jobs.csv', mode='a',index=False,header=False)        \n","        #df"]}],"metadata":{"kernelspec":{"display_name":"Python 3.11.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"dca6e724a2d65d6e3c91e9208e4ccbf805543924cecd268a6285ca4202b63e15"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}